# -*- coding: utf-8 -*-
"""diffusion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yVh3nyuUHfc-BDrBXtYwTcA-7fr2w96b
"""

# Colab-ready conditional DDPM notebook (sketch -> photo)
# Run cell-by-cell in Colab. Replace zip path like in your previous code.

# 1) Mount drive & unzip (same as yours)
from google.colab import drive
drive.mount('/content/drive')

import zipfile, os
zip_path = "/content/drive/MyDrive/archive (6).zip"   # <- change if needed
extract_dir = "/content/dataset"
if not os.path.exists(extract_dir):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)
print("‚úÖ Extracted to:", extract_dir)
print("Subfolders:", os.listdir(extract_dir))

!pip install -q --upgrade pip
!pip install -q "huggingface_hub==0.25.2" "diffusers==0.28.0" transformers accelerate albumentations kornia einops

import os
import math
import random
from pathlib import Path
from tqdm.auto import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from PIL import Image
import numpy as np
import albumentations as A
from albumentations.pytorch import ToTensorV2

import torchvision.utils as vutils     # üî• required for make_grid
from diffusers import UNet2DModel, DDPMScheduler

from google.colab import drive
drive.mount('/content/drive')

OUTPUT_DIR = "/content/drive/MyDrive/sketch2real_out"
os.makedirs(OUTPUT_DIR, exist_ok=True)
print("Saving checkpoints to:", OUTPUT_DIR)

# ---------- Config ----------
IMAGE_SIZE = 64           # 128 or 256 (smaller -> faster)
BATCH_SIZE = 16
EPOCHS = 150
LEARNING_RATE = 1e-4
LOG_INTERVAL = 200         # iterations
SAVE_EVERY = 5            # epochs

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

class SketchPhotoDataset(Dataset):
    def __init__(self, sketch_dir, photo_dir, image_size=IMAGE_SIZE, augment=False):
        self.sketch_paths = sorted([
            str(Path(sketch_dir)/p) for p in os.listdir(sketch_dir)
            if p.lower().endswith(('.png','.jpg','.jpeg'))
        ])
        self.photo_paths = sorted([
            str(Path(photo_dir)/p) for p in os.listdir(photo_dir)
            if p.lower().endswith(('.png','.jpg','.jpeg'))
        ])
        assert len(self.sketch_paths) == len(self.photo_paths), \
            f"Mismatch pairs: {len(self.sketch_paths)} vs {len(self.photo_paths)}"

        base_trans = [A.Resize(image_size, image_size)]
        if augment:
            base_trans += [A.HorizontalFlip(p=0.5), A.RandomRotate90(p=0.3)]
        self.trans = A.Compose(base_trans + [
            A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),
            ToTensorV2()
        ])

    def __len__(self):
        return len(self.sketch_paths)

    def __getitem__(self, idx):
        s = np.array(Image.open(self.sketch_paths[idx]).convert("RGB"))
        p = np.array(Image.open(self.photo_paths[idx]).convert("RGB"))
        s = self.trans(image=s)['image']
        p = self.trans(image=p)['image']
        return s, p

train_sketch_dir = "/content/dataset/train/sketches"
train_photo_dir  = "/content/dataset/train/photos"
val_sketch_dir   = "/content/dataset/val/sketches"
val_photo_dir    = "/content/dataset/val/photos"

train_ds = SketchPhotoDataset(train_sketch_dir, train_photo_dir, augment=True)
val_ds   = SketchPhotoDataset(val_sketch_dir, val_photo_dir, augment=False)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)

print(f"‚úÖ Train size: {len(train_ds)} | Val size: {len(val_ds)}")

# UNet backbone from diffusers ‚Äî conditional via concatenation
unet = UNet2DModel(
    sample_size=IMAGE_SIZE,
    in_channels=6,         # noisy image (3) + sketch (3)
    out_channels=3,        # predict noise
    layers_per_block=2,
    block_out_channels=(64, 128, 128, 256),
    attention_head_dim=None,
).to(device)

noise_scheduler = DDPMScheduler(num_train_timesteps=500)
optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)

def save_grid(sketches, photos, preds, path, nrow=4):
    grid_sk = vutils.make_grid(sketches, nrow=nrow, normalize=True, value_range=(-1,1))
    grid_ph = vutils.make_grid(photos, nrow=nrow, normalize=True, value_range=(-1,1))
    grid_pd = vutils.make_grid(preds, nrow=nrow, normalize=True, value_range=(-1,1))
    grid = torch.cat([grid_sk, grid_ph, grid_pd], dim=1)
    vutils.save_image(grid, path)

import glob

def find_latest_checkpoint(ckpt_dir):
    paths = glob.glob(os.path.join(ckpt_dir, "*.pt"))
    if not paths: return None
    return sorted(paths, key=os.path.getmtime, reverse=True)[0]

latest = find_latest_checkpoint(OUTPUT_DIR)
start_epoch, global_step = 1, 0
scaler_state = None

if latest:
    print("üîÅ Found checkpoint:", latest)
    ckpt = torch.load(latest, map_location=device)
    unet.load_state_dict(ckpt["model_state_dict"])
    optimizer.load_state_dict(ckpt["optimizer_state_dict"])
    if "scaler_state_dict" in ckpt:
        scaler_state = ckpt["scaler_state_dict"]
    start_epoch = ckpt.get("epoch", 1) + 1
    global_step = ckpt.get("global_step", 0)
    print(f"Resuming from epoch {start_epoch}")
else:
    print("üÜï No checkpoint found ‚Äî training from scratch.")

scaler = amp.GradScaler("cuda", enabled=(device.type == "cuda"))
if scaler_state: scaler.load_state_dict(scaler_state)

for epoch in range(start_epoch, EPOCHS + 1):
    unet.train()
    running_loss = 0.0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{EPOCHS}")

    for sketches, photos in pbar:
        sketches, photos = sketches.to(device), photos.to(device)

        batch_size = photos.size(0)
        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,
                                  (batch_size,), device=device).long()

        noise = torch.randn_like(photos)
        noisy_photos = noise_scheduler.add_noise(photos, noise, timesteps)
        model_input = torch.cat([noisy_photos, sketches], dim=1)

        optimizer.zero_grad(set_to_none=True)
        with amp.autocast("cuda", enabled=(device.type == "cuda")):
            pred_noise = unet(model_input, timesteps).sample
            loss = F.mse_loss(pred_noise, noise)

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()
        if (global_step + 1) % LOG_INTERVAL == 0:
            avg = running_loss / LOG_INTERVAL
            pbar.set_postfix({"loss": f"{avg:.4f}"})
            running_loss = 0.0

        global_step += 1

    # --- Save checkpoint and validation sample every 5 epochs ---
    if epoch % SAVE_EVERY == 0 or epoch == EPOCHS:
        ckpt_path = os.path.join(OUTPUT_DIR, f"unet_epoch_{epoch}.pt")
        torch.save({
            "epoch": epoch,
            "model_state_dict": unet.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scaler_state_dict": scaler.state_dict(),
            "global_step": global_step
        }, ckpt_path)
        print(f"‚úÖ Saved checkpoint: {ckpt_path}")
unet.eval()
with torch.no_grad():
    sketches_v, photos_v = next(iter(val_loader))
    sketches_v, photos_v = sketches_v.to(device), photos_v.to(device)
    sample = torch.randn_like(photos_v).to(device)

    for t in reversed(range(noise_scheduler.config.num_train_timesteps)):
        # üîπ use scalar for scheduler
        t_tensor = torch.tensor(t, device=device, dtype=torch.long)
        # üîπ batch tensor for model input
        t_batch = torch.tensor([t] * sample.size(0), device=device, dtype=torch.long)

        model_input = torch.cat([sample, sketches_v], dim=1)
        model_output = unet(model_input, t_batch).sample

        # ‚úÖ correct scheduler call
        step_out = noise_scheduler.step(model_output, t_tensor, sample)
        sample = step_out.prev_sample

    save_path = os.path.join(OUTPUT_DIR, f"sample_epoch_{epoch}.png")
    save_grid(sketches_v.cpu(), photos_v.cpu(), sample.cpu(), save_path)
    print("üñºÔ∏è Saved visual sample:", save_path)

import glob, os, torch

def find_latest_checkpoint(ckpt_dir):
    paths = glob.glob(os.path.join(ckpt_dir, "*.pt"))
    if not paths:
        return None
    return sorted(paths, key=os.path.getmtime, reverse=True)[0]

latest_ckpt = find_latest_checkpoint(OUTPUT_DIR)
start_epoch, global_step = 1, 0
scaler_state = None

if latest_ckpt:
    print("‚úÖ Found checkpoint:", latest_ckpt)
    ckpt = torch.load(latest_ckpt, map_location=device)
    unet.load_state_dict(ckpt["model_state_dict"])
    optimizer.load_state_dict(ckpt["optimizer_state_dict"])
    scaler_state = ckpt.get("scaler_state_dict", None)
    start_epoch = ckpt.get("epoch", 1) + 1
    global_step = ckpt.get("global_step", 0)
    print(f"üîÅ Resuming from epoch {start_epoch} (step {global_step})")
else:
    print("üÜï No checkpoint found ‚Äî starting fresh.")

from torch import amp  # make sure amp is imported

scaler = amp.GradScaler(device.type, enabled=(device.type == "cuda"))
if scaler_state:
    scaler.load_state_dict(scaler_state)


for epoch in range(start_epoch, EPOCHS + 1):
    unet.train()
    running_loss = 0.0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{EPOCHS}")

    for sketches, photos in pbar:
        sketches, photos = sketches.to(device), photos.to(device)

        batch_size = photos.size(0)
        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,
                                  (batch_size,), device=device).long()

        noise = torch.randn_like(photos)
        noisy_photos = noise_scheduler.add_noise(photos, noise, timesteps)
        model_input = torch.cat([noisy_photos, sketches], dim=1)

        optimizer.zero_grad(set_to_none=True)
        with amp.autocast("cuda", enabled=(device.type == "cuda")):
            pred_noise = unet(model_input, timesteps).sample
            loss = F.mse_loss(pred_noise, noise)

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()
        if (global_step + 1) % LOG_INTERVAL == 0:
            avg = running_loss / LOG_INTERVAL
            pbar.set_postfix({"loss": f"{avg:.4f}"})
            running_loss = 0.0

        global_step += 1

    # --- Save checkpoint and validation sample every 5 epochs ---
    if epoch % SAVE_EVERY == 0 or epoch == EPOCHS:
        ckpt_path = os.path.join(OUTPUT_DIR, f"unet_epoch_{epoch}.pt")
        torch.save({
            "epoch": epoch,
            "model_state_dict": unet.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scaler_state_dict": scaler.state_dict(),
            "global_step": global_step
        }, ckpt_path)
        print(f"‚úÖ Saved checkpoint: {ckpt_path}")
unet.eval()
with torch.no_grad():
    sketches_v, photos_v = next(iter(val_loader))
    sketches_v, photos_v = sketches_v.to(device), photos_v.to(device)
    sample = torch.randn_like(photos_v).to(device)

    for t in reversed(range(noise_scheduler.config.num_train_timesteps)):
        # üîπ use scalar for scheduler
        t_tensor = torch.tensor(t, device=device, dtype=torch.long)
        # üîπ batch tensor for model input
        t_batch = torch.tensor([t] * sample.size(0), device=device, dtype=torch.long)

        model_input = torch.cat([sample, sketches_v], dim=1)
        model_output = unet(model_input, t_batch).sample

        # ‚úÖ correct scheduler call
        step_out = noise_scheduler.step(model_output, t_tensor, sample)
        sample = step_out.prev_sample

    save_path = os.path.join(OUTPUT_DIR, f"sample_epoch_{epoch}.png")
    save_grid(sketches_v.cpu(), photos_v.cpu(), sample.cpu(), save_path)
    print("üñºÔ∏è Saved visual sample:", save_path)

from torch import amp  # make sure amp is imported

scaler = amp.GradScaler(device.type, enabled=(device.type == "cuda"))
if scaler_state:
    scaler.load_state_dict(scaler_state)


for epoch in range(start_epoch, EPOCHS + 1):
    unet.train()
    running_loss = 0.0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{EPOCHS}")

    for sketches, photos in pbar:
        sketches, photos = sketches.to(device), photos.to(device)

        batch_size = photos.size(0)
        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,
                                  (batch_size,), device=device).long()

        noise = torch.randn_like(photos)
        noisy_photos = noise_scheduler.add_noise(photos, noise, timesteps)
        model_input = torch.cat([noisy_photos, sketches], dim=1)

        optimizer.zero_grad(set_to_none=True)
        with amp.autocast("cuda", enabled=(device.type == "cuda")):
            pred_noise = unet(model_input, timesteps).sample
            loss = F.mse_loss(pred_noise, noise)

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()
        if (global_step + 1) % LOG_INTERVAL == 0:
            avg = running_loss / LOG_INTERVAL
            pbar.set_postfix({"loss": f"{avg:.4f}"})
            running_loss = 0.0

        global_step += 1

    # --- Save checkpoint and validation sample every 5 epochs ---
    if epoch % SAVE_EVERY == 0 or epoch == EPOCHS:
        ckpt_path = os.path.join(OUTPUT_DIR, f"unet_epoch_{epoch}.pt")
        torch.save({
            "epoch": epoch,
            "model_state_dict": unet.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scaler_state_dict": scaler.state_dict(),
            "global_step": global_step
        }, ckpt_path)
        print(f"‚úÖ Saved checkpoint: {ckpt_path}")
unet.eval()
with torch.no_grad():
    sketches_v, photos_v = next(iter(val_loader))
    sketches_v, photos_v = sketches_v.to(device), photos_v.to(device)
    sample = torch.randn_like(photos_v).to(device)

    for t in reversed(range(noise_scheduler.config.num_train_timesteps)):
        # üîπ use scalar for scheduler
        t_tensor = torch.tensor(t, device=device, dtype=torch.long)
        # üîπ batch tensor for model input
        t_batch = torch.tensor([t] * sample.size(0), device=device, dtype=torch.long)

        model_input = torch.cat([sample, sketches_v], dim=1)
        model_output = unet(model_input, t_batch).sample

        # ‚úÖ correct scheduler call
        step_out = noise_scheduler.step(model_output, t_tensor, sample)
        sample = step_out.prev_sample

    save_path = os.path.join(OUTPUT_DIR, f"sample_epoch_{epoch}.png")
    save_grid(sketches_v.cpu(), photos_v.cpu(), sample.cpu(), save_path)
    print("üñºÔ∏è Saved visual sample:", save_path)

# Save each predicted image individually
pred_dir = os.path.join(OUTPUT_DIR, "preds")
os.makedirs(pred_dir, exist_ok=True)

for i in range(sample.size(0)):
    img = (sample[i].clamp(-1, 1) + 1) / 2   # back to [0,1]
    img = transforms.ToPILImage()(img.cpu())
    img.save(os.path.join(pred_dir, f"pred_{epoch}_{i}.png"))

from torch import amp  # make sure amp is imported

scaler = amp.GradScaler(device.type, enabled=(device.type == "cuda"))
if scaler_state:
    scaler.load_state_dict(scaler_state)


for epoch in range(start_epoch, EPOCHS + 1):
    unet.train()
    running_loss = 0.0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{EPOCHS}")

    for sketches, photos in pbar:
        sketches, photos = sketches.to(device), photos.to(device)

        batch_size = photos.size(0)
        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,
                                  (batch_size,), device=device).long()

        noise = torch.randn_like(photos)
        noisy_photos = noise_scheduler.add_noise(photos, noise, timesteps)
        model_input = torch.cat([noisy_photos, sketches], dim=1)

        optimizer.zero_grad(set_to_none=True)
        with amp.autocast("cuda", enabled=(device.type == "cuda")):
            pred_noise = unet(model_input, timesteps).sample
            loss = F.mse_loss(pred_noise, noise)

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()
        if (global_step + 1) % LOG_INTERVAL == 0:
            avg = running_loss / LOG_INTERVAL
            pbar.set_postfix({"loss": f"{avg:.4f}"})
            running_loss = 0.0

        global_step += 1

    # --- Save checkpoint and validation sample every 5 epochs ---
    if epoch % SAVE_EVERY == 0 or epoch == EPOCHS:
        ckpt_path = os.path.join(OUTPUT_DIR, f"unet_epoch_{epoch}.pt")
        torch.save({
            "epoch": epoch,
            "model_state_dict": unet.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scaler_state_dict": scaler.state_dict(),
            "global_step": global_step
        }, ckpt_path)
        print(f"‚úÖ Saved checkpoint: {ckpt_path}")
unet.eval()
with torch.no_grad():
    sketches_v, photos_v = next(iter(val_loader))
    sketches_v, photos_v = sketches_v.to(device), photos_v.to(device)
    sample = torch.randn_like(photos_v).to(device)

    for t in reversed(range(noise_scheduler.config.num_train_timesteps)):
        # üîπ use scalar for scheduler
        t_tensor = torch.tensor(t, device=device, dtype=torch.long)
        # üîπ batch tensor for model input
        t_batch = torch.tensor([t] * sample.size(0), device=device, dtype=torch.long)

        model_input = torch.cat([sample, sketches_v], dim=1)
        model_output = unet(model_input, t_batch).sample

        # ‚úÖ correct scheduler call
        step_out = noise_scheduler.step(model_output, t_tensor, sample)
        sample = step_out.prev_sample

    save_path = os.path.join(OUTPUT_DIR, f"sample_epoch_{epoch}.png")
    save_grid(sketches_v.cpu(), photos_v.cpu(), sample.cpu(), save_path)
    print("üñºÔ∏è Saved visual sample:", save_path)

from torch import amp  # make sure amp is imported

scaler = amp.GradScaler(device.type, enabled=(device.type == "cuda"))
if scaler_state:
    scaler.load_state_dict(scaler_state)


for epoch in range(start_epoch, EPOCHS + 1):
    unet.train()
    running_loss = 0.0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{EPOCHS}")

    for sketches, photos in pbar:
        sketches, photos = sketches.to(device), photos.to(device)

        batch_size = photos.size(0)
        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,
                                  (batch_size,), device=device).long()

        noise = torch.randn_like(photos)
        noisy_photos = noise_scheduler.add_noise(photos, noise, timesteps)
        model_input = torch.cat([noisy_photos, sketches], dim=1)

        optimizer.zero_grad(set_to_none=True)
        with amp.autocast("cuda", enabled=(device.type == "cuda")):
            pred_noise = unet(model_input, timesteps).sample
            loss = F.mse_loss(pred_noise, noise)

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()
        if (global_step + 1) % LOG_INTERVAL == 0:
            avg = running_loss / LOG_INTERVAL
            pbar.set_postfix({"loss": f"{avg:.4f}"})
            running_loss = 0.0

        global_step += 1

    # --- Save checkpoint and validation sample every 5 epochs ---
    if epoch % SAVE_EVERY == 0 or epoch == EPOCHS:
        ckpt_path = os.path.join(OUTPUT_DIR, f"unet_epoch_{epoch}.pt")
        torch.save({
            "epoch": epoch,
            "model_state_dict": unet.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scaler_state_dict": scaler.state_dict(),
            "global_step": global_step
        }, ckpt_path)
        print(f"‚úÖ Saved checkpoint: {ckpt_path}")
unet.eval()
with torch.no_grad():
    sketches_v, photos_v = next(iter(val_loader))
    sketches_v, photos_v = sketches_v.to(device), photos_v.to(device)
    sample = torch.randn_like(photos_v).to(device)

    for t in reversed(range(noise_scheduler.config.num_train_timesteps)):
        # üîπ use scalar for scheduler
        t_tensor = torch.tensor(t, device=device, dtype=torch.long)
        # üîπ batch tensor for model input
        t_batch = torch.tensor([t] * sample.size(0), device=device, dtype=torch.long)

        model_input = torch.cat([sample, sketches_v], dim=1)
        model_output = unet(model_input, t_batch).sample

        # ‚úÖ correct scheduler call
        step_out = noise_scheduler.step(model_output, t_tensor, sample)
        sample = step_out.prev_sample

    save_path = os.path.join(OUTPUT_DIR, f"sample_epoch_{epoch}.png")
    save_grid(sketches_v.cpu(), photos_v.cpu(), sample.cpu(), save_path)
    print("üñºÔ∏è Saved visual sample:", save_path)

import torch
import os

# Reload the latest checkpoint
ckpt_path = "/content/drive/MyDrive/sketch2real_out/unet_epoch_150.pt"
ckpt = torch.load(ckpt_path, map_location=device)

unet.load_state_dict(ckpt["model_state_dict"])
print("‚úÖ Loaded trained UNet from checkpoint")

epoch = ckpt["epoch"]

unet.eval()
with torch.no_grad():
    sketches_v, photos_v = next(iter(val_loader))
    sketches_v, photos_v = sketches_v.to(device), photos_v.to(device)

    # Random starting noise (same shape as photos)
    sample = torch.randn_like(photos_v).to(device)

    # DDPM reverse process
    for t in reversed(range(noise_scheduler.config.num_train_timesteps)):
        t_tensor = torch.tensor(t, device=device, dtype=torch.long)
        t_batch = torch.tensor([t] * sample.size(0), device=device, dtype=torch.long)

        model_input = torch.cat([sample, sketches_v], dim=1)
        model_output = unet(model_input, t_batch).sample

        step_out = noise_scheduler.step(model_output, t_tensor, sample)
        sample = step_out.prev_sample

    # save result
    save_path = os.path.join(OUTPUT_DIR, f"sample_epoch_{epoch}.png")
    save_grid(sketches_v.cpu(), photos_v.cpu(), sample.cpu(), save_path)

print("üñºÔ∏è Saved visual sample:", save_path)

import torch
import os
import torchvision.utils as vutils
from PIL import Image

# ------------------------------
# Save ONLY predicted images
# ------------------------------
def save_pred_only(preds, path, nrow=4):
    # preds: (B, 3, H, W)
    grid = vutils.make_grid(preds, nrow=nrow, normalize=True, value_range=(-1, 1))
    img = grid.permute(1, 2, 0).cpu().numpy()
    img = (img * 255).astype("uint8")
    Image.fromarray(img).save(path)


# ------------------------------
# 1. Load the trained checkpoint
# ------------------------------
ckpt_path = "/content/drive/MyDrive/sketch2real_out/unet_epoch_150.pt"
ckpt = torch.load(ckpt_path, map_location=device)

unet.load_state_dict(ckpt["model_state_dict"])
print("‚úÖ Loaded UNet checkpoint!")

epoch = ckpt["epoch"]


# ------------------------------
# 2. Run inference
# ------------------------------
unet.eval()
with torch.no_grad():
    sketches_v, photos_v = next(iter(val_loader))
    sketches_v, photos_v = sketches_v.to(device), photos_v.to(device)

    # Start from random noise (same as DDPM)
    sample = torch.randn_like(photos_v).to(device)

    # DDPM sampling loop
    for t in reversed(range(noise_scheduler.config.num_train_timesteps)):
        t_scalar = torch.tensor(t, device=device, dtype=torch.long)
        t_batch = torch.tensor([t] * sample.size(0), device=device, dtype=torch.long)

        model_input = torch.cat([sample, sketches_v], dim=1)
        model_output = unet(model_input, t_batch).sample

        step_out = noise_scheduler.step(model_output, t_scalar, sample)
        sample = step_out.prev_sample

    # ------------------------------
    # 3. Save ONLY predictions
    # ------------------------------
    save_path = os.path.join(OUTPUT_DIR, f"prediction_only_epoch_{epoch}.png")
    save_pred_only(sample.cpu(), save_path)
    print("üñºÔ∏è Saved prediction:", save_path)

import torch
import os
import torchvision.utils as vutils
from PIL import Image

# ------------------------------------------------
# Helper: convert tensor ‚Üí image and save
# ------------------------------------------------
def save_single_image(tensor_img, path):
    img = tensor_img.clamp(-1, 1)               # ensure valid range
    img = (img + 1) / 2                         # [-1,1] ‚Üí [0,1]
    img = img.permute(1, 2, 0).cpu().numpy()    # CHW‚ÜíHWC
    img = (img * 255).astype("uint8")
    Image.fromarray(img).save(path)


# ------------------------------------------------
# 1. Load checkpoint
# ------------------------------------------------
ckpt_path = "/content/drive/MyDrive/sketch2real_out/unet_epoch_150.pt"
ckpt = torch.load(ckpt_path, map_location=device)
unet.load_state_dict(ckpt["model_state_dict"])
print("‚úÖ Loaded UNet checkpoint!")

epoch = ckpt["epoch"]


# ------------------------------------------------
# 2. Generate predictions for 5 samples
# ------------------------------------------------
unet.eval()
with torch.no_grad():

    # Take EXACTLY 5 samples from validation set
    sketches_v, photos_v = next(iter(val_loader))
    sketches_v = sketches_v[:5].to(device)
    photos_v = photos_v[:5].to(device)    # also needed for evaluation later

    sample = torch.randn_like(photos_v).to(device)

    # DDPM generation loop
    for t in reversed(range(noise_scheduler.config.num_train_timesteps)):
        t_scalar = torch.tensor(t, device=device)
        t_batch = torch.tensor([t]*sample.size(0), device=device)

        model_input = torch.cat([sample, sketches_v], dim=1)
        noise_pred = unet(model_input, t_batch).sample

        step = noise_scheduler.step(noise_pred, t_scalar, sample)
        sample = step.prev_sample

    # ------------------------------------------------
    # 3. Save 5 individual predicted images
    # ------------------------------------------------
    for i in range(5):
        pred_img = sample[i].cpu()
        save_path = os.path.join(OUTPUT_DIR, f"pred_{i+1}.png")
        save_single_image(pred_img, save_path)
        print(f"üìå Saved {save_path}")

print("\nüéâ Done! All 5 predicted images saved.")

!pip install lpips
!pip install scikit-image

import torch
import os
import numpy as np
import lpips
from skimage.metrics import peak_signal_noise_ratio, structural_similarity
from torchvision.utils import save_image
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -----------------------------
# 1. LOAD TRAINED CHECKPOINT
# -----------------------------
ckpt_path = "/content/drive/MyDrive/sketch2real_out/unet_epoch_150.pt"
ckpt = torch.load(ckpt_path, map_location=device)

unet.load_state_dict(ckpt["model_state_dict"])
print("‚úÖ Loaded UNet checkpoint")

epoch = ckpt["epoch"]

# -----------------------------
# 2. LPIPS MODEL (FAST ALEXNET)
# -----------------------------
lpips_model = lpips.LPIPS(net='alex').to(device)
print("‚úÖ LPIPS model loaded (AlexNet)")

# -----------------------------
# 3. RUN PREDICTION ON 5 IMAGES
# -----------------------------
unet.eval()
pred_images = []
gt_images = []
sketch_images = []

with torch.no_grad():
    for i in range(5):
        sketches_v, photos_v = next(iter(val_loader))
        sketches_v, photos_v = sketches_v.to(device), photos_v.to(device)

        sketch_images.append(sketches_v[0].cpu())
        gt_images.append(photos_v[0].cpu())

        sample = torch.randn_like(photos_v).to(device)

        for t in reversed(range(noise_scheduler.config.num_train_timesteps)):
            t_tensor = torch.tensor(t, device=device)
            t_batch = torch.tensor([t]*sample.size(0), device=device)

            model_input = torch.cat([sample, sketches_v], dim=1)
            model_output = unet(model_input, t_batch).sample

            step_out = noise_scheduler.step(model_output, t_tensor, sample)
            sample = step_out.prev_sample

        predicted = sample[0].cpu()
        pred_images.append(predicted)

        save_path = f"/content/drive/MyDrive/sketch2real_out/pred_{i+1}.png"
        save_image((predicted + 1) / 2, save_path)   # convert to 0‚Äì1 for saving
        print(f"üñºÔ∏è Saved predicted image {i+1} ‚Üí {save_path}")

print("‚úÖ All 5 images generated.")

# -----------------------------
# 4. EVALUATION METRICS
# -----------------------------
psnr_list = []
ssim_list = []
lpips_list = []

for i in range(5):
    gt = gt_images[i]
    pred = pred_images[i]

    # normalize to numpy [0,1]
    gt_np = ((gt.permute(1,2,0).numpy() + 1) / 2).astype(np.float32)
    pred_np = ((pred.permute(1,2,0).numpy() + 1) / 2).astype(np.float32)

    # ---- PSNR ----
    psnr = peak_signal_noise_ratio(gt_np, pred_np, data_range=1.0)
    psnr_list.append(psnr)

    # ---- SSIM ----
    ssim = structural_similarity(gt_np, pred_np, channel_axis=2, data_range=1.0)
    ssim_list.append(ssim)

    # ---- LPIPS ----
    gt_t = gt.unsqueeze(0).to(device)
    pred_t = pred.unsqueeze(0).to(device)
    lp = lpips_model(gt_t, pred_t).item()
    lpips_list.append(lp)

print("\n====== üìä FINAL METRICS (5 images) ======")
print(f"PSNR:  {np.mean(psnr_list):.4f}")
print(f"SSIM:  {np.mean(ssim_list):.4f}")
print(f"LPIPS: {np.mean(lpips_list):.4f}")

ckpt = torch.load("/content/drive/MyDrive/sketch2real_out/unet_epoch_150.pt", map_location=device)

unet.load_state_dict(ckpt["model_state_dict"])
print("‚úî Loaded UNET weights from checkpoint 150")

torch.save(unet.state_dict(),
           "/content/drive/MyDrive/sketch2real_out/unet_final_weights.pt")

print("üéâ Saved final model weights!")