# -*- coding: utf-8 -*-
"""transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VhxPHpvXXgq4r1H2e7-VO8MSMAk6bQdB
"""

from google.colab import drive
drive.mount('/content/drive')

# ============================================================
# 1. Setup Kaggle API credentials
# ============================================================
from google.colab import files
import os

print("üìÅ Please upload your kaggle.json file (from https://www.kaggle.com > Account > Create API Token)")
files.upload()

# Move kaggle.json to the correct location
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json

# Test Kaggle setup
!kaggle datasets list | head -n 5

# ============================================================
# 2. Download dataset from Kaggle
# ============================================================
!kaggle datasets download -d almightyj/person-face-sketches

# ============================================================
# 3. Extract the dataset
# ============================================================
import zipfile

zip_path = "person-face-sketches.zip"
extract_dir = "/content/dataset"

if not os.path.exists(extract_dir):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)

print("‚úÖ Dataset extracted to:", extract_dir)
print("üìÇ Subfolders:", os.listdir(extract_dir))

# ============================================================
# 4. Inspect contents (train/test/val)
# ============================================================
import glob

for split in ['train', 'test', 'val']:
    photo_dir = os.path.join(extract_dir, split, 'photos')
    sketch_dir = os.path.join(extract_dir, split, 'sketches')

    num_photos = len(glob.glob(os.path.join(photo_dir, '*')))
    num_sketches = len(glob.glob(os.path.join(sketch_dir, '*')))

    print(f"\nüìÅ {split.upper()} SET:")
    print(f"  Photos: {num_photos}")
    print(f"  Sketches: {num_sketches}")

print("\n‚úÖ All done!")

!pip install torch torchvision tqdm Pillow matplotlib

import torch
torch.cuda.is_available(), torch.cuda.get_device_name(0)

!pip install torch torchvision matplotlib scikit-image tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, datasets
import matplotlib.pyplot as plt
from skimage.metrics import structural_similarity as ssim
from skimage.metrics import peak_signal_noise_ratio as psnr
import numpy as np
from tqdm import tqdm
import os
from PIL import Image

class SketchPhotoDataset(Dataset):
    def __init__(self, sketch_dir, photo_dir, transform=None):
        self.sketch_dir = sketch_dir
        self.photo_dir = photo_dir
        self.sketches = sorted(os.listdir(sketch_dir))
        self.photos = sorted(os.listdir(photo_dir))
        self.transform = transform

    def __len__(self):
        return len(self.sketches)

    def __getitem__(self, idx):
        sketch_path = os.path.join(self.sketch_dir, self.sketches[idx])
        photo_path = os.path.join(self.photo_dir, self.photos[idx])

        sketch = Image.open(sketch_path).convert('L')  # grayscale
        photo = Image.open(photo_path).convert('RGB')

        if self.transform:
            sketch = self.transform(sketch)
            photo = self.transform(photo)

        return sketch, photo

transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor()
])

train_dataset = SketchPhotoDataset(
    sketch_dir="/content/dataset/train/sketches",
    photo_dir="/content/dataset/train/photos",
    transform=transform
)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
from torchmetrics.functional import peak_signal_noise_ratio as psnr, structural_similarity_index_measure as ssim

# =============================
# Vision Transformer from Scratch
# =============================
class PatchEmbedding(nn.Module):
    def __init__(self, img_size=128, patch_size=16, in_chans=1, embed_dim=256):
        super().__init__()
        self.patch_embed = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        num_patches = (img_size // patch_size) ** 2
        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, embed_dim))

    def forward(self, x):
        x = self.patch_embed(x)                     # [B, embed_dim, H/patch, W/patch]
        x = x.flatten(2).transpose(1, 2)            # [B, num_patches, embed_dim]
        return x + self.pos_embed


class TransformerEncoderBlock(nn.Module):
    def __init__(self, embed_dim=256, num_heads=8, mlp_dim=512, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, mlp_dim),
            nn.GELU(),
            nn.Linear(mlp_dim, embed_dim)
        )

    def forward(self, x):
        attn_out, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))
        x = x + attn_out
        x = x + self.mlp(self.norm2(x))
        return x


class ViTRegressor(nn.Module):
    def __init__(self, img_size=128, patch_size=16, in_chans=1, embed_dim=256, num_layers=6, num_heads=8):
        super().__init__()
        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)
        self.encoder = nn.Sequential(*[
            TransformerEncoderBlock(embed_dim, num_heads) for _ in range(num_layers)
        ])
        self.fc_enc = nn.Linear(embed_dim, embed_dim)
        self.fc_dec = nn.Sequential(
            nn.Linear(embed_dim, 3 * img_size * img_size),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.patch_embed(x)
        x = self.encoder(x)
        x = torch.mean(x, dim=1)  # Global average pooling
        z = self.fc_enc(x)
        out = self.fc_dec(z)
        return out.view(-1, 3, 128, 128)

!pip install torchmetrics

# =============================
# FAST Training Loop with AMP + Optimized Metrics + Drive Save
# =============================

import torch
import torch.nn as nn
from torchmetrics.image import LearnedPerceptualImagePatchSimilarity
from torchmetrics.functional import peak_signal_noise_ratio as psnr, structural_similarity_index_measure as ssim
from tqdm import tqdm
import os

# Device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Enable cuDNN Optimizations
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.fastest = True

# Model / Optimizer / Loss
model = ViTRegressor().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.MSELoss()

# Mixed Precision
scaler = torch.cuda.amp.GradScaler()

# LPIPS (very slow ‚Üí compute occasionally)
lpips_metric = LearnedPerceptualImagePatchSimilarity(net_type='vgg').to(device)

# Google Drive checkpoint directory
DRIVE_SAVE_DIR = "/content/drive/MyDrive/vit_sketch2real_checkpoints"
os.makedirs(DRIVE_SAVE_DIR, exist_ok=True)

# Training
epochs = 100

for epoch in range(epochs):
    model.train()
    running_loss = running_psnr = running_ssim = running_lpips = 0

    # Compute heavy metrics only on some epochs
    compute_metrics = (epoch % 2 == 0)       # every 2 epochs
    compute_lpips = (epoch % 5 == 0)         # every 5 epochs

    for sketch, photo in tqdm(train_loader):
        sketch, photo = sketch.to(device), photo.to(device)
        optimizer.zero_grad()

        # =============================
        # MIXED PRECISION FORWARD + BACKWARD
        # =============================
        with torch.cuda.amp.autocast():
            recon = model(sketch)
            loss = criterion(recon, photo)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()

        # =============================
        # FAST METRICS (OPTIONAL)
        # =============================
        if compute_metrics:
            with torch.no_grad():
                running_psnr += psnr(recon, photo).item()
                running_ssim += ssim(recon, photo).item()

        if compute_lpips:
            with torch.no_grad():
                running_lpips += lpips_metric((recon + 1) / 2, (photo + 1) / 2).item()

    # =============================
    # PRINT SUMMARY FOR THIS EPOCH
    # =============================
    print(f"\nEpoch [{epoch+1}/{epochs}]")
    print(f"Loss: {running_loss/len(train_loader):.4f}")

    if compute_metrics:
        print(f"PSNR: {running_psnr/len(train_loader):.4f}")
        print(f"SSIM: {running_ssim/len(train_loader):.4f}")

    if compute_lpips:
        print(f"LPIPS: {running_lpips/len(train_loader):.4f}")

    # =============================
    # SAVE CHECKPOINT
    # =============================
    save_path = f"{DRIVE_SAVE_DIR}/epoch_{epoch+1}.pth"
    torch.save({
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': running_loss / len(train_loader)
    }, save_path)

    print(f"üíæ Saved checkpoint ‚Üí {save_path}")

import matplotlib.pyplot as plt
import torch

model.eval()
with torch.no_grad():
    sketch, photo = next(iter(train_loader))
    sketch, photo = sketch.to(device), photo.to(device)
    generated = model(sketch).cpu()

# Show results
plt.figure(figsize=(12, 6))
for i in range(5):
    # Sketch (input)
    plt.subplot(3, 5, i + 1)
    plt.imshow(sketch[i].cpu().squeeze(), cmap='gray')
    plt.title("Sketch")
    plt.axis('off')

    # Real Photo (target)
    plt.subplot(3, 5, i + 6)
    plt.imshow(photo[i].cpu().permute(1, 2, 0))
    plt.title("Real")
    plt.axis('off')

    # Generated Photo (output)
    plt.subplot(3, 5, i + 11)
    plt.imshow(generated[i].permute(1, 2, 0))
    plt.title("Generated")
    plt.axis('off')

plt.tight_layout()
plt.show()

import os

save_dir = "/content/drive/MyDrive/Sketch2Real_Models"
os.makedirs(save_dir, exist_ok=True)
print(f"üìÅ Models will be saved in: {save_dir}")

model_path = os.path.join(save_dir, "transfoermer_sketch2real.pth")
torch.save(model.state_dict(), model_path)
print(f" Model weights saved to Drive at: {model_path}")

torch.save(model.state_dict(), "final_model.pth")

checkpoint_path = os.path.join(save_dir, "transfer_checkpoint.pth")
torch.save({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, checkpoint_path)
print(f"Checkpoint saved to: {checkpoint_path}")

# from google.colab import drive
# drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Re-create the model
model = ViTRegressor().to(device)

# Load saved weights
model.load_state_dict(torch.load("/content/drive/MyDrive/Sketch2Real_Models/transfoermer_sketch2real.pth",
                                 map_location=device))
model.eval()

print("‚úÖ Model loaded from Drive and ready for inference!")

import torch
import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Re-create model & optimizer
model = ViTRegressor().to(device) # Changed from VAE() to ViTRegressor()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) # Changed learning rate

# Load last checkpoint
checkpoint_path = "/content/drive/MyDrive/Sketch2Real_Models/transfer_checkpoint.pth" # Changed checkpoint path
checkpoint = torch.load(checkpoint_path, map_location=device)

model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
start_epoch = checkpoint['epoch']  # This will be 50

print(f"‚úÖ Resuming from epoch {start_epoch}") # Removed val_loss from print statement

# Set total epochs you want
total_epochs = 90

# --- Continue Training ---
# Assuming you have a validation loader and a loss function defined elsewhere
# For this example, we'll just continue training for the remaining epochs
for epoch in range(start_epoch + 1, total_epochs): # Start from the next epoch
    model.train()
    running_loss, running_psnr, running_ssim = 0, 0, 0 # Reset metrics for each epoch
    for sketch, photo in tqdm(train_loader):
        sketch, photo = sketch.to(device), photo.to(device)
        optimizer.zero_grad()
        recon = model(sketch)
        loss = criterion(recon, photo) # Using criterion from the training loop
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        running_psnr += psnr(recon, photo).item()
        running_ssim += ssim(recon, photo).item()


    print(f"Epoch [{epoch+1}/{total_epochs}] | Loss: {running_loss/len(train_loader):.4f} | "
          f"PSNR: {running_psnr/len(train_loader):.4f} | SSIM: {running_ssim/len(train_loader):.4f}")

    # Save checkpoint after each epoch
    torch.save({
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
    }, os.path.join(save_dir, f"transfer_checkpoint_epoch_{epoch+1}.pth"))
    print(f"Checkpoint saved for epoch {epoch+1}")

print("Training finished.")

